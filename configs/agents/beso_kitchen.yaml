_target_: beso.agents.diffusion_agents.beso_agent.BesoAgent # agents.joint_ibc_agent.PlanarRobotJointIBCAgent  # agents.joint_distribution_ebm_agent.PlanarBotJointEBMAgent
_recursive_: false

defaults:
 # diffusion_gpt augmented_score_model augmented_seq_transformer_score_model diffusion_gpt film_diffusion_gpt film_diffusion_gpt film_seq_transformer_score_model
  - model: diffusion_gpt # film_diffusion_gpt 
  - input_encoder: no_encoder

optimization:
  _target_: torch.optim.AdamW
  lr: 1e-4 
  betas: [0.9, 0.999]

lr_scheduler:
  _target_: torch.optim.lr_scheduler.StepLR
  step_size: 100
  gamma: 0.99

obs_modalities: ${obs_modalities}
goal_modalities: ${goal_modalities}
target_modality: ${target_modality}
train_method: ${train_method}
max_epochs: ${max_epochs}

goal_conditioned: ${goal_conditioning}
pred_last_action_only: False
eval_every_n_steps: ${eval_every_n_steps}
max_train_steps: ${max_train_steps}
num_sampling_steps: ${n_timesteps}

# current sampler types:
# 'lms', 'euler', 'heun', 'ancestral', 'dpm', 'euler_ancestral', 'dpmpp_2s_ancestral', 'dpmpp_2m','dpm_fast', 'dpm_adaptive',
sampler_type: 'ddim'
sigma_data: ${sigma_data}
rho: 5.

sigma_min: 0.005 
sigma_max: 1 
# sample density stuff
sigma_sample_density_type:  'loglogistic' # 'loglogistic' # 'lognormal' # 'loglogistic' 
# these two are only relevant for lognormal distribution if chosen
sigma_sample_density_mean: -0.6 #  -1.2 #-3.56 # -0.61
sigma_sample_density_std: 1.6 # 1.2 # 2.42  # -2 # 1.58
use_ema: ${use_ema}
decay: ${decay}
device: ${device}
update_ema_every_n_steps: ${update_ema_every_n_steps}
goal_window_size: ${future_seq_length}
window_size: ${window_size}

patience: 80 # interval for early stopping during epoch training